Open a CSV in Colab:
from google.colab import files
uploaded = files.upload()

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
path="/content/drive/MyDrive/ML-CAT/exp-6.csv"
df = pd.read_csv(path)
df


PCA:
import pandas as pd
import numpy as np
l = [[2,1],[3,5],[4,3],[5,6],[6,7],[7,8]]
df = pd.DataFrame(l)
print(df)

df.columns =['X', 'Y']
df.columns

df[df.columns[0]][2]

for i in df.columns:
  print(df[i].tolist())

mean_df = df.mean(axis=0)
print(mean_df)

def compute_covariance(x,y):
  res = 0
  xmean = sum(x)/len(x)
  ymean = sum(y)/len(y)
  for i in range(len(x)):
    res+=((x[i]-xmean)*(y[i]-ymean))
  return res/len(x)

x = df['X'].tolist()
y = df['Y'].tolist()
cov = compute_covariance(x,x)
print(cov)

cov_mat = []
for i in df.columns:
  t = []
  for j in df.columns:
    t.append(compute_covariance(df[i].tolist(),df[j].tolist()))
  cov_mat.append(t)
print(cov_mat)

w, v = np.linalg.eig(cov_mat)
print('Eigen value: ',w)
print('Eigen vector: ',v)



import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

df = pd.read_csv(r'C:\Users\vijay s\Desktop\heart_data.csv')
features = df.columns.tolist()
df.head()

encoder = OrdinalEncoder()
data_encoded = encoder.fit_transform(df[features])
df_encoded = pd.DataFrame(data_encoded, columns=features)
df_encoded

X = df_encoded.iloc[:, 1:13]
y = df_encoded.iloc[:, -1]
print(X.head(),y.head())

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 142)

pca = PCA(n_components = 6)

X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)
pca_result=pd.DataFrame(X_test)
pca_result

scaler = StandardScaler()
scaled_df=df.copy()
scaled_df=pd.DataFrame(scaler.fit_transform(scaled_df), columns=scaled_df.columns)
pca_fit = pca.fit(scaled_df)

PC_values = np.arange(pca.n_components_) + 1
plt.plot(PC_values, pca.explained_variance_ratio_, 'o-', linewidth=2, color='blue')
plt.title('Scree Plot')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.show()

print(pca.explained_variance_ratio_)





Factor Analysis:
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.preprocessing import OrdinalEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.decomposition import FactorAnalysis as fa
from factor_analyzer import FactorAnalyzer
from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity
from factor_analyzer.factor_analyzer import calculate_kmo

df = pd.read_csv(r'C:\Users\vijay s\Desktop\exp3_ml.csv')
features = df.columns.tolist()
df.head()

encoder = OrdinalEncoder()
data_encoded = encoder.fit_transform(df[features])
df_encoded = pd.DataFrame(data_encoded, columns=features)
df_encoded

x = df_encoded.iloc[:, 1:13]
y = df_encoded.iloc[:, -1]
z = x.copy()
print(x.head(),y.head())

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 142)

#PCA
pca = PCA(n_components =6)

x_train = pca.fit_transform(x_train)
x_test = pca.transform(x_test)
pca_result=pd.DataFrame(x_test)
pca_result.head()

plt.title('Scree Plot')
plt.xlabel('Principal Component')
plt.ylabel('Variance Explained')
plt.plot(np.arange(pca.n_components_) + 1, pca.explained_variance_ratio_, 'o-', linewidth=2, color='green')
plt.show()

print(pca.explained_variance_ratio_)

#Factor Analysis

chi2,p = calculate_bartlett_sphericity(df)
chi2,p

#If the p test statistic value is less than 0.05, we can decide that the correlation is not an Identical matrix 
#i.e. correlation is present among the variables with a 95% confidence level.

kmo_vars,kmo_model = calculate_kmo(df)
kmo_model

#KMO score is always between 0 to 1 and values more than 0.6 are much appreciated.

fa = FactorAnalyzer(rotation = None,impute = "drop",n_factors=df.shape[1])
fa.fit(df)

ev,_ = fa.get_eigenvalues()
ev

plt.scatter(range(1,df.shape[1]+1),ev)
plt.plot(range(1,df.shape[1]+1),ev)
plt.title('Scree Plot')
plt.xlabel('Factors')
plt.ylabel('Eigen Value')
plt.grid()

#Here, only for 3-factors eigenvalues are greater than one. 
#It means we need to choose only 3 factors (or unobserved variables).

#Without Rotation with 3 factors
fa1 = FactorAnalyzer(3)
fa1.fit(df)
print("Without Rotation:\n",fa1.loadings_)

print(pd.DataFrame(fa1.get_factor_variance(),index=['Variance','Proportional Var','Cumulative Var']))

print(pd.DataFrame(fa1.get_communalities(),index=df.columns,columns=['Communalities']))

#Communality tells you what proportion of the variable's variance is a result of either: 
#The principal components or. The correlations between each variable and individual factors
#Rotations donâ€™t have any influence over the communality of the variables.
#i.e)Communality value for both Rotation and Without Rotation will be the same.

matrix1 = pd.DataFrame(fa1.loadings_, index = list(df.columns), columns = ['Factor 1', 'Factor 2', 'Factor 3'])
matrix1

#Factor 1 => diabetes, highbp, age
#Factor 2 => generalhealth, mentalhealth, physicalhealth, diffinwalk 
#Factor 3 => education, income

#['varimax', 'oblimax', 'quartimax', 'equamax', 'geomin_ort', 'promax', 'oblimin', 'quartimin', 'geomin_obl', None]

#With Rotation with 3 factors
fa2 = FactorAnalyzer(3, rotation='oblimin')
fa2.fit(df)
print("With Rotation:\n",fa2.loadings_)

print(pd.DataFrame(fa2.get_factor_variance(),index=['Variance','Proportional Var','Cumulative Var']))

matrix2 = pd.DataFrame(fa2.loadings_, index = list(df.columns), columns = ['Factor 1', 'Factor 2', 'Factor 3'])
matrix2

#Factor 1 => generalhealth, mentalhealth, physicalhealth, diffinwalk
#Factor 2 => highbp, age
#Factor 3 => education, income






Decision Tree:
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math
from sklearn.tree import DecisionTreeClassifier
from sklearn import tree

df=pd.read_csv(r"C:\Users\vijay s\Desktop\exp-6.csv")
df = df.dropna()
df

attributes = [ i for i in df]
attributes

counts = {}
for k in attributes:
  for i,j in zip(df["profitable"],df[k]):
      if(k not in counts):
          counts[k] = {"total":0}
      if(j not in counts[k]):
        counts[k][j] = {"total":0}
      if(i not in counts[k][j]):
        counts[k][j][i] = 0
      counts[k][j][i] += 1
      counts[k][j]["total"] += 1
      counts[k]["total"] += 1
print(counts)

def decision(x,y):
  if x==0 or y==0:
    return 0
  value = -x * math.log(x,2) -y * math.log(y,2)
  return value

main=decision(counts["profitable"]["yes"]["total"]/counts["profitable"]["total"],counts["profitable"]["no"]["total"]/counts["profitable"]["total"])

def cls(b,i):
  if("yes" not in counts[b][i].keys()):
    counts[b][i]["yes"]=0
  if("no" not in counts[b][i].keys()):
    counts[b][i]["no"]=0
  return -(counts[b][i]["total"]/counts[b]["total"])*decision((counts[b][i]["yes"]/counts[b][i]["total"]),(counts[b][i]["no"]/counts[b][i]["total"]))

def calculateMinusValue(b):
  uniques = [ i for i in counts[b]]
  uniques = uniques[1:]
  values = [ cls(b,i) for i in uniques]
  return values

def informationGain(b):
  return main + sum(calculateMinusValue(b))

arr = [informationGain(i) for i in attributes[0:5]]
arr

#Information Gain of Price = 0.016313165825732057
#Information Gain of Maintenance = 0.18905266854301617
#Information Gain of Capacity = 0.19813134764391394
#Information Gain of Airbag = 0.00723448672483451
#Total Entropy = 0.9940302114769565
#Attribute 'Capacity' has highest Information Gain among all other attributes

y=df["profitable"]
y

df['price'].replace(['low','med', 'high'],[0,1,2], inplace=True)
df['airbag'].replace(['yes','no'],[0,1], inplace=True)
df['maintenance'].replace(['low','med','high'],[0,1,2], inplace=True)
df['profitable'].replace(['yes','no'],[1,0],inplace=True)
df

dtree=DecisionTreeClassifier()
X=df.drop("profitable",axis=1)
X

dtree = DecisionTreeClassifier()
model=dtree.fit(X,y)
print(tree.export_text(dtree))

feature=list(X.columns)
y=y.astype(str)

fig=plt.figure(figsize=(18,10))
_=tree.plot_tree(dtree,feature_names=feature,class_names=y,filled=True)